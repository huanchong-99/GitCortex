//! Workflow API Routes

use std::{collections::HashMap, path::PathBuf, sync::Arc};

use axum::{
    Json, Router,
    extract::{Path, Query, State},
    response::Json as ResponseJson,
    routing::{get, post, put},
};
use db::models::{
    CliType, CreateWorkflowRequest, InlineModelConfig, ModelConfig, SlashCommandPreset, Terminal,
    Workflow, WorkflowCommand, WorkflowTask,
    project::Project,
    workflow::{CreateTerminalRequest, CreateWorkflowTaskRequest},
};
use deployment::Deployment;
use serde::{Deserialize, Serialize};
use serde_json::json;
use services::services::{
    cc_switch::CCSwitchService,
    git::GitServiceError,
    orchestrator::{BusMessage, TerminalCoordinator},
    terminal::TerminalLauncher,
};
use utils::{response::ApiResponse, text};
use uuid::Uuid;

// Import DTOs
use crate::routes::workflows_dto::{WorkflowDetailDto, WorkflowListItemDto};
use crate::{DeploymentImpl, error::ApiError};

// ============================================================================
// Request/Response Types
// ============================================================================

/// Workflow Detail Response
#[derive(Debug, Serialize)]
pub struct WorkflowDetailResponse {
    #[serde(flatten)]
    pub workflow: Workflow,
    pub tasks: Vec<WorkflowTaskDetailResponse>,
    pub commands: Vec<WorkflowCommandWithPreset>,
}

/// Workflow Task Detail Response
#[derive(Debug, Serialize)]
pub struct WorkflowTaskDetailResponse {
    #[serde(flatten)]
    pub task: WorkflowTask,
    pub terminals: Vec<Terminal>,
}

/// Workflow Command with Preset
#[derive(Debug, Serialize)]
pub struct WorkflowCommandWithPreset {
    #[serde(flatten)]
    pub command: WorkflowCommand,
    pub preset: SlashCommandPreset,
}

/// Update Workflow Status Request
#[derive(Debug, Deserialize)]
pub struct UpdateWorkflowStatusRequest {
    pub status: String,
}

/// Recovery Response
#[derive(Debug, Serialize)]
pub struct RecoveryResponse {
    pub message: String,
}

/// Merge Workflow Request
#[derive(Debug, Deserialize)]
pub struct MergeWorkflowRequest {
    pub merge_strategy: Option<String>,
}

const WORKFLOW_STATUSES: [&str; 9] = [
    "created",
    "starting",
    "ready",
    "running",
    "paused",
    "merging",
    "completed",
    "failed",
    "cancelled",
];

const MERGE_ALLOWED_WORKFLOW_STATUSES: [&str; 2] = ["completed", "merging"];

// ============================================================================
// Route Definition
// ============================================================================

/// Create workflows router
pub fn workflows_routes() -> Router<DeploymentImpl> {
    Router::new()
        .route("/", get(list_workflows).post(create_workflow))
        .route("/recover", post(recover_workflows))
        .route("/{workflow_id}", get(get_workflow).delete(delete_workflow))
        .route("/{workflow_id}/status", put(update_workflow_status))
        .route("/{workflow_id}/prepare", post(prepare_workflow))
        .route("/{workflow_id}/start", post(start_workflow))
        .route("/{workflow_id}/pause", post(pause_workflow))
        .route("/{workflow_id}/stop", post(stop_workflow))
        .route(
            "/{workflow_id}/prompts/respond",
            post(submit_prompt_response),
        )
        .route("/{workflow_id}/merge", post(merge_workflow))
        .route("/{workflow_id}/tasks", get(list_workflow_tasks))
        .route(
            "/{workflow_id}/tasks/{task_id}/status",
            put(update_task_status),
        )
        .route(
            "/{workflow_id}/tasks/{task_id}/terminals",
            get(list_task_terminals),
        )
}

fn is_known_workflow_status(status: &str) -> bool {
    WORKFLOW_STATUSES.contains(&status)
}

fn can_merge_from_workflow_status(status: &str) -> bool {
    MERGE_ALLOWED_WORKFLOW_STATUSES.contains(&status)
}

fn is_valid_workflow_status_transition(current: &str, next: &str) -> bool {
    if current == next {
        return is_known_workflow_status(current);
    }

    matches!(
        (current, next),
        ("created", "starting")
            | ("created", "failed")
            | ("created", "cancelled")
            | ("starting", "ready")
            | ("starting", "failed")
            | ("starting", "cancelled")
            | ("ready", "running")
            | ("ready", "failed")
            | ("ready", "cancelled")
            | ("running", "paused")
            | ("running", "completed")
            | ("running", "failed")
            | ("running", "cancelled")
            | ("paused", "ready")
            | ("paused", "running")
            | ("paused", "failed")
            | ("paused", "cancelled")
            | ("completed", "merging")
            | ("completed", "created")
            | ("merging", "completed")
            | ("merging", "failed")
            | ("failed", "starting")
            | ("failed", "created")
            | ("failed", "cancelled")
            | ("cancelled", "created")
    )
}

fn validate_workflow_status_transition(current: &str, next: &str) -> Result<(), ApiError> {
    if !is_known_workflow_status(next) {
        return Err(ApiError::BadRequest(format!(
            "Invalid workflow status '{}', expected one of: {:?}",
            next, WORKFLOW_STATUSES
        )));
    }

    if !is_known_workflow_status(current) {
        return Err(ApiError::Conflict(format!(
            "Cannot transition workflow from unknown status '{}': expected one of: {:?}",
            current, WORKFLOW_STATUSES
        )));
    }

    if !is_valid_workflow_status_transition(current, next) {
        return Err(ApiError::Conflict(format!(
            "Invalid workflow status transition: '{}' -> '{}'",
            current, next
        )));
    }

    Ok(())
}

fn validate_task_workflow_scope(task: &WorkflowTask, workflow_id: &str) -> Result<(), ApiError> {
    if task.workflow_id != workflow_id {
        return Err(ApiError::BadRequest(
            "Task does not belong to this workflow".to_string(),
        ));
    }

    Ok(())
}

// ============================================================================
// Route Handlers
// ============================================================================

/// Validate create workflow request
pub fn validate_create_request(req: &CreateWorkflowRequest) -> Result<(), ApiError> {
    // Validate project_id
    if req.project_id.trim().is_empty() {
        return Err(ApiError::BadRequest("projectId is required".to_string()));
    }

    // Validate workflow name
    if req.name.trim().is_empty() {
        return Err(ApiError::BadRequest("name is required".to_string()));
    }

    // Validate tasks is not empty
    if req.tasks.is_empty() {
        return Err(ApiError::BadRequest("tasks must not be empty".to_string()));
    }

    // Validate each task
    for (task_index, task) in req.tasks.iter().enumerate() {
        if task.name.trim().is_empty() {
            return Err(ApiError::BadRequest(format!(
                "task[{}].name is required",
                task_index
            )));
        }

        if task.terminals.is_empty() {
            return Err(ApiError::BadRequest(format!(
                "task[{}].terminals must not be empty",
                task_index
            )));
        }

        // Validate each terminal
        for (terminal_index, terminal) in task.terminals.iter().enumerate() {
            if terminal.cli_type_id.trim().is_empty() {
                return Err(ApiError::BadRequest(format!(
                    "task[{}].terminal[{}].cliTypeId is required",
                    task_index, terminal_index
                )));
            }

            if terminal.model_config_id.trim().is_empty() {
                return Err(ApiError::BadRequest(format!(
                    "task[{}].terminal[{}].modelConfigId is required",
                    task_index, terminal_index
                )));
            }
        }
    }

    // Validate commands if provided
    if let Some(ref commands) = req.commands {
        for (cmd_index, cmd) in commands.iter().enumerate() {
            if cmd.preset_id.trim().is_empty() {
                return Err(ApiError::BadRequest(format!(
                    "commands[{}].presetId is required",
                    cmd_index
                )));
            }

            // Validate custom_params JSON format if provided
            if let Some(ref params) = cmd.custom_params {
                if !params.trim().is_empty() {
                    serde_json::from_str::<serde_json::Value>(params).map_err(|_| {
                        ApiError::BadRequest(format!(
                            "commands[{}].customParams must be valid JSON",
                            cmd_index
                        ))
                    })?;
                }
            }
        }
    }

    Ok(())
}

/// Validate CLI types and model configs exist in database
/// If model_config_id doesn't exist but inline model_config is provided,
/// automatically create a new ModelConfig record.
async fn validate_cli_and_model_configs(
    pool: &sqlx::SqlitePool,
    req: &CreateWorkflowRequest,
) -> Result<(), ApiError> {
    // Collect unique model_config_id references with CLI type and inline config
    let mut model_config_refs: HashMap<String, (String, Option<InlineModelConfig>)> =
        HashMap::new();

    // Helper to track model config references
    let mut track_ref = |cli_type_id: &str,
                         model_config_id: &str,
                         inline: Option<&InlineModelConfig>|
     -> Result<(), ApiError> {
        match model_config_refs.get_mut(model_config_id) {
            Some((existing_cli_type_id, existing_inline)) => {
                // Validate same model_config_id is used with same CLI type
                if existing_cli_type_id != cli_type_id {
                    return Err(ApiError::BadRequest(format!(
                        "Model config {model_config_id} used with multiple CLI types: {existing_cli_type_id} and {cli_type_id}"
                    )));
                }
                // Use inline config if not already set
                if existing_inline.is_none() {
                    *existing_inline = inline.cloned();
                }
            }
            None => {
                model_config_refs.insert(
                    model_config_id.to_string(),
                    (cli_type_id.to_string(), inline.cloned()),
                );
            }
        }
        Ok(())
    };

    // Track merge terminal config
    track_ref(
        &req.merge_terminal_config.cli_type_id,
        &req.merge_terminal_config.model_config_id,
        req.merge_terminal_config.model_config.as_ref(),
    )?;

    // Track error terminal config if present
    if let Some(error_config) = &req.error_terminal_config {
        track_ref(
            &error_config.cli_type_id,
            &error_config.model_config_id,
            error_config.model_config.as_ref(),
        )?;
    }

    // Track all task terminals
    for task in &req.tasks {
        for terminal in &task.terminals {
            track_ref(
                &terminal.cli_type_id,
                &terminal.model_config_id,
                terminal.model_config.as_ref(),
            )?;
        }
    }

    // Validate each unique model_config_id
    for (model_config_id, (cli_type_id, inline)) in model_config_refs {
        // Validate CLI type exists
        let cli_type = CliType::find_by_id(pool, &cli_type_id)
            .await
            .map_err(|e| ApiError::Internal(format!("Database error: {e}")))?;

        if cli_type.is_none() {
            return Err(ApiError::BadRequest(format!(
                "CLI type not found: {cli_type_id}"
            )));
        }

        // Validate model config exists or create from inline data
        let model_config = ModelConfig::find_by_id(pool, &model_config_id)
            .await
            .map_err(|e| ApiError::Internal(format!("Database error: {e}")))?;

        let model_config = match model_config {
            Some(mc) => mc,
            None => {
                // Model config not found - try to create from inline data
                let inline = inline.ok_or_else(|| ApiError::BadRequest(format!(
                    "Model config not found: {model_config_id}. Provide inline modelConfig to auto-create."
                )))?;

                // Create custom model config from inline data
                ModelConfig::create_custom(
                    pool,
                    &model_config_id,
                    &cli_type_id,
                    &inline.display_name,
                    &inline.model_id,
                )
                .await
                .map_err(|e| ApiError::Internal(format!("Failed to create model config: {e}")))?
            }
        };

        // Validate model config belongs to the CLI type
        if model_config.cli_type_id != cli_type_id {
            return Err(ApiError::BadRequest(format!(
                "Model config {model_config_id} does not belong to CLI type {cli_type_id}"
            )));
        }
    }

    Ok(())
}

/// GET /api/workflows?project_id=xxx
/// List workflows for a project
async fn list_workflows(
    State(deployment): State<DeploymentImpl>,
    Query(params): Query<HashMap<String, String>>,
) -> Result<ResponseJson<ApiResponse<Vec<WorkflowListItemDto>>>, ApiError> {
    let project_id_str = params
        .get("project_id")
        .ok_or_else(|| ApiError::BadRequest("project_id is required".to_string()))?;

    // Parse project_id as UUID
    let project_id = Uuid::parse_str(project_id_str)
        .map_err(|_| ApiError::BadRequest("project_id must be a valid UUID".to_string()))?;

    // Use optimized query that returns counts in a single database call
    let workflows_with_counts =
        Workflow::find_by_project_with_counts(&deployment.db().pool, project_id).await?;

    // Convert to DTOs
    let dtos: Vec<WorkflowListItemDto> = workflows_with_counts
        .into_iter()
        .map(|w| WorkflowListItemDto {
            id: w.id,
            project_id: w.project_id.to_string(),
            name: w.name,
            description: w.description,
            status: w.status,
            created_at: w.created_at.to_rfc3339(),
            updated_at: w.updated_at.to_rfc3339(),
            tasks_count: w.tasks_count as i32,
            terminals_count: w.terminals_count as i32,
        })
        .collect();

    Ok(ResponseJson(ApiResponse::success(dtos)))
}

/// POST /api/workflows
/// Create workflow
async fn create_workflow(
    State(deployment): State<DeploymentImpl>,
    Json(req): Json<CreateWorkflowRequest>,
) -> Result<ResponseJson<ApiResponse<WorkflowDetailDto>>, ApiError> {
    // Validate request structure
    validate_create_request(&req)?;

    // Parse and validate project_id as UUID
    let project_id = Uuid::parse_str(&req.project_id)
        .map_err(|_| ApiError::BadRequest("projectId must be a valid UUID".to_string()))?;

    // Validate CLI types and model configs exist in database
    validate_cli_and_model_configs(&deployment.db().pool, &req).await?;

    let now = chrono::Utc::now();
    let workflow_id = Uuid::new_v4().to_string();

    // Log workflow creation details
    let total_terminals: usize = req.tasks.iter().map(|task| task.terminals.len()).sum();
    tracing::info!(
        workflow_id = %workflow_id,
        project_id = %project_id,
        tasks = req.tasks.len(),
        terminals = total_terminals,
        "creating workflow"
    );

    // 1. Create workflow with encrypted API key
    let mut workflow = Workflow {
        id: workflow_id.clone(),
        project_id,
        name: req.name,
        description: req.description,
        status: "created".to_string(),
        use_slash_commands: req.use_slash_commands,
        orchestrator_enabled: req.orchestrator_config.is_some(),
        orchestrator_api_type: req.orchestrator_config.as_ref().map(|c| c.api_type.clone()),
        orchestrator_base_url: req.orchestrator_config.as_ref().map(|c| c.base_url.clone()),
        orchestrator_api_key: None, // Will be set encrypted below
        orchestrator_model: req.orchestrator_config.as_ref().map(|c| c.model.clone()),
        error_terminal_enabled: req.error_terminal_config.is_some(),
        error_terminal_cli_id: req
            .error_terminal_config
            .as_ref()
            .map(|c| c.cli_type_id.clone()),
        error_terminal_model_id: req
            .error_terminal_config
            .as_ref()
            .map(|c| c.model_config_id.clone()),
        merge_terminal_cli_id: req.merge_terminal_config.cli_type_id.clone(),
        merge_terminal_model_id: req.merge_terminal_config.model_config_id.clone(),
        target_branch: req.target_branch.unwrap_or_else(|| "main".to_string()),
        ready_at: None,
        started_at: None,
        completed_at: None,
        created_at: now,
        updated_at: now,
    };

    // Encrypt and store API key if provided
    if let Some(orch_config) = &req.orchestrator_config {
        workflow
            .set_api_key(&orch_config.api_key)
            .map_err(|e| ApiError::BadRequest(format!("Failed to encrypt API key: {e}")))?;
    }

    // 2. Prepare tasks and terminals for transactional creation
    let mut task_rows: Vec<(WorkflowTask, Vec<Terminal>)> = Vec::new();
    let mut existing_branches: Vec<String> = Vec::new();

    // Collect existing branch names for conflict detection
    // In a real scenario, we'd query the git repository for existing branches
    // For now, we collect branches that will be created in this batch
    for (_task_index, task_req) in req.tasks.iter().enumerate() {
        let task_id = Uuid::new_v4().to_string();

        // Generate branch name using slugify with conflict detection
        let branch = if let Some(custom_branch) = &task_req.branch {
            // Use custom branch name if provided
            custom_branch.clone()
        } else {
            // Auto-generate branch name: workflow/{workflow_id}/{slugified-task-name}
            // Check against already-generated branches in this batch
            let base_branch = format!(
                "workflow/{}/{}",
                workflow_id,
                text::git_branch_id(&task_req.name)
            );
            let mut candidate = base_branch.clone();
            let mut counter = 2;

            while existing_branches.contains(&candidate) {
                candidate = format!("{}-{}", base_branch, counter);
                counter += 1;
            }

            candidate
        };

        // Track this branch to avoid conflicts within the same batch
        existing_branches.push(branch.clone());

        let task = WorkflowTask {
            id: task_id.clone(),
            workflow_id: workflow_id.clone(),
            vk_task_id: task_req
                .id
                .as_deref()
                .and_then(|id| Uuid::parse_str(id).ok()),
            name: task_req.name.clone(),
            description: task_req.description.clone(),
            branch,
            status: "pending".to_string(),
            order_index: task_req.order_index,
            started_at: None,
            completed_at: None,
            created_at: now,
            updated_at: now,
        };

        let mut terminals: Vec<Terminal> = Vec::new();

        for terminal_req in &task_req.terminals {
            let mut terminal = Terminal {
                id: Uuid::new_v4().to_string(),
                workflow_task_id: task_id.clone(),
                cli_type_id: terminal_req.cli_type_id.clone(),
                model_config_id: terminal_req.model_config_id.clone(),
                custom_base_url: terminal_req.custom_base_url.clone(),
                custom_api_key: None, // Will be set encrypted below
                role: terminal_req.role.clone(),
                role_description: terminal_req.role_description.clone(),
                order_index: terminal_req.order_index,
                status: "not_started".to_string(),
                process_id: None,
                pty_session_id: None,
                session_id: None,
                execution_process_id: None,
                vk_session_id: None,
                auto_confirm: terminal_req.auto_confirm,
                last_commit_hash: None,
                last_commit_message: None,
                started_at: None,
                completed_at: None,
                created_at: now,
                updated_at: now,
            };

            // Encrypt and store API key if provided
            if let Some(custom_api_key) = terminal_req.custom_api_key.as_deref() {
                terminal.set_custom_api_key(custom_api_key).map_err(|e| {
                    ApiError::BadRequest(format!("Failed to encrypt terminal API key: {e}"))
                })?;
            }

            terminals.push(terminal);
        }

        task_rows.push((task, terminals));
    }

    // 3. Execute transactional creation (workflow + tasks + terminals)
    Workflow::create_with_tasks(&deployment.db().pool, &workflow, task_rows)
        .await
        .map_err(|e| ApiError::BadRequest(format!("Failed to create workflow: {e}")))?;

    // 4. Create slash command associations (after workflow exists)
    let mut commands: Vec<WorkflowCommand> = Vec::new();
    if let Some(command_reqs) = req.commands {
        for (index, cmd_req) in command_reqs.iter().enumerate() {
            let index = i32::try_from(index)
                .map_err(|_| ApiError::BadRequest("Command index overflow".to_string()))?;

            // Validate custom_params is valid JSON if provided
            if let Some(ref params) = cmd_req.custom_params {
                if !params.trim().is_empty() {
                    // Validate JSON format
                    serde_json::from_str::<serde_json::Value>(params).map_err(|_| {
                        ApiError::BadRequest(format!(
                            "Invalid JSON in custom_params for preset {}",
                            cmd_req.preset_id
                        ))
                    })?;
                }
            }

            WorkflowCommand::create(
                &deployment.db().pool,
                &workflow_id,
                &cmd_req.preset_id,
                index,
                cmd_req.custom_params.as_deref(),
            )
            .await?;
        }
        commands = WorkflowCommand::find_by_workflow(&deployment.db().pool, &workflow_id).await?;
    }

    // 5. Get command preset details
    let all_presets = SlashCommandPreset::find_all(&deployment.db().pool).await?;
    let commands_with_presets: Vec<(WorkflowCommand, SlashCommandPreset)> = commands
        .into_iter()
        .filter_map(|cmd| {
            all_presets
                .iter()
                .find(|p| p.id == cmd.preset_id)
                .map(|preset| (cmd, preset.clone()))
        })
        .collect();

    // 6. Load tasks with terminals
    let tasks = WorkflowTask::find_by_workflow(&deployment.db().pool, &workflow_id).await?;
    let mut task_details: Vec<(WorkflowTask, Vec<Terminal>)> = Vec::new();
    for task in &tasks {
        let terminals = Terminal::find_by_task(&deployment.db().pool, &task.id).await?;
        task_details.push((task.clone(), terminals));
    }

    // Convert to DTO
    let dto = WorkflowDetailDto::from_workflow_with_terminals(
        &workflow,
        &task_details,
        &commands_with_presets,
    );

    Ok(ResponseJson(ApiResponse::success(dto)))
}

/// GET /api/workflows/:workflow_id
/// Get workflow details
async fn get_workflow(
    State(deployment): State<DeploymentImpl>,
    Path(workflow_id): Path<String>,
) -> Result<ResponseJson<ApiResponse<WorkflowDetailDto>>, ApiError> {
    // Get workflow
    let workflow = Workflow::find_by_id(&deployment.db().pool, &workflow_id)
        .await?
        .ok_or_else(|| ApiError::NotFound("Workflow not found".to_string()))?;

    // Get tasks and terminals
    let tasks = WorkflowTask::find_by_workflow(&deployment.db().pool, &workflow_id).await?;

    // Get commands with presets
    let commands = WorkflowCommand::find_by_workflow(&deployment.db().pool, &workflow_id).await?;
    let all_presets = SlashCommandPreset::find_all(&deployment.db().pool).await?;
    let commands_with_presets: Vec<(WorkflowCommand, SlashCommandPreset)> = commands
        .into_iter()
        .filter_map(|cmd| {
            all_presets
                .iter()
                .find(|p| p.id == cmd.preset_id)
                .map(|preset| (cmd, preset.clone()))
        })
        .collect();

    // Load terminals for each task
    let mut task_details: Vec<(WorkflowTask, Vec<Terminal>)> = Vec::new();
    for task in &tasks {
        let terminals = Terminal::find_by_task(&deployment.db().pool, &task.id).await?;
        task_details.push((task.clone(), terminals));
    }

    // Convert to DTO
    let dto = WorkflowDetailDto::from_workflow_with_terminals(
        &workflow,
        &task_details,
        &commands_with_presets,
    );

    Ok(ResponseJson(ApiResponse::success(dto)))
}

/// DELETE /api/workflows/:workflow_id
/// Delete workflow
async fn delete_workflow(
    State(deployment): State<DeploymentImpl>,
    Path(workflow_id): Path<String>,
) -> Result<ResponseJson<ApiResponse<()>>, ApiError> {
    stop_workflow_runtime_if_running(
        &deployment,
        &workflow_id,
        "deleting workflow",
        "Failed to delete workflow",
    )
    .await?;
    cleanup_workflow_terminals(&deployment, &workflow_id, "deleting workflow").await?;
    Workflow::delete(&deployment.db().pool, &workflow_id).await?;
    Ok(ResponseJson(ApiResponse::success(())))
}

/// PUT /api/workflows/:workflow_id/status
/// Update workflow status
async fn update_workflow_status(
    State(deployment): State<DeploymentImpl>,
    Path(workflow_id): Path<String>,
    Json(req): Json<UpdateWorkflowStatusRequest>,
) -> Result<ResponseJson<ApiResponse<()>>, ApiError> {
    let workflow = Workflow::find_by_id(&deployment.db().pool, &workflow_id)
        .await?
        .ok_or_else(|| ApiError::NotFound("Workflow not found".to_string()))?;

    let target_status = req.status.trim().to_string();
    validate_workflow_status_transition(&workflow.status, &target_status)?;

    Workflow::update_status(&deployment.db().pool, &workflow_id, &target_status).await?;
    Ok(ResponseJson(ApiResponse::success(())))
}

async fn rollback_prepare_failure(deployment: &DeploymentImpl, workflow_id: &str, reason: &str) {
    tracing::warn!(
        workflow_id = %workflow_id,
        reason = %reason,
        "Rolling back workflow prepare state"
    );

    let terminals = match Terminal::find_by_workflow(&deployment.db().pool, workflow_id).await {
        Ok(terminals) => terminals,
        Err(e) => {
            tracing::warn!(
                workflow_id = %workflow_id,
                error = %e,
                "Failed to list terminals during prepare rollback"
            );
            Vec::new()
        }
    };
    let workflow_topic = format!("workflow:{}", workflow_id);

    for terminal in terminals {
        if let Err(e) = deployment
            .process_manager()
            .kill_terminal(&terminal.id)
            .await
        {
            tracing::warn!(
                terminal_id = %terminal.id,
                workflow_id = %workflow_id,
                error = %e,
                "Failed to kill terminal process during prepare rollback"
            );
        }

        deployment.prompt_watcher().unregister(&terminal.id).await;

        if let Err(e) =
            Terminal::update_process(&deployment.db().pool, &terminal.id, None, None).await
        {
            tracing::warn!(
                terminal_id = %terminal.id,
                workflow_id = %workflow_id,
                error = %e,
                "Failed to clear terminal process binding during prepare rollback"
            );
        }

        if let Err(e) =
            Terminal::update_session(&deployment.db().pool, &terminal.id, None, None).await
        {
            tracing::warn!(
                terminal_id = %terminal.id,
                workflow_id = %workflow_id,
                error = %e,
                "Failed to clear terminal session binding during prepare rollback"
            );
        }

        if let Err(e) =
            Terminal::update_status(&deployment.db().pool, &terminal.id, "not_started").await
        {
            tracing::warn!(
                terminal_id = %terminal.id,
                workflow_id = %workflow_id,
                error = %e,
                "Failed to reset terminal status during prepare rollback"
            );
        } else {
            let message = BusMessage::TerminalStatusUpdate {
                workflow_id: workflow_id.to_string(),
                terminal_id: terminal.id.clone(),
                status: "not_started".to_string(),
            };
            if let Err(e) = deployment
                .message_bus()
                .publish(&workflow_topic, message.clone())
                .await
            {
                tracing::warn!(
                    workflow_id = %workflow_id,
                    terminal_id = %terminal.id,
                    error = %e,
                    "Failed to publish terminal rollback status"
                );
            }
            if let Err(e) = deployment.message_bus().broadcast(message) {
                tracing::warn!(
                    workflow_id = %workflow_id,
                    terminal_id = %terminal.id,
                    error = %e,
                    "Failed to broadcast terminal rollback status"
                );
            }
        }
    }

    if let Err(e) = Workflow::update_status(&deployment.db().pool, workflow_id, "failed").await {
        tracing::warn!(
            workflow_id = %workflow_id,
            error = %e,
            "Failed to set workflow status during prepare rollback"
        );
    }
}

async fn resolve_workflow_working_dir(
    deployment: &DeploymentImpl,
    workflow: &Workflow,
) -> Result<PathBuf, ApiError> {
    let project = Project::find_by_id(&deployment.db().pool, workflow.project_id)
        .await?
        .ok_or_else(|| ApiError::NotFound("Project not found".to_string()))?;

    if let Some(dir) = project
        .default_agent_working_dir
        .as_deref()
        .map(str::trim)
        .filter(|dir| !dir.is_empty())
    {
        return Ok(PathBuf::from(dir));
    }

    let repo_working_dir: Option<String> = sqlx::query_scalar(
        r#"
        SELECT r.path
        FROM repos r
        INNER JOIN project_repos pr ON pr.repo_id = r.id
        WHERE pr.project_id = ?
        ORDER BY r.display_name ASC
        LIMIT 1
        "#,
    )
    .bind(workflow.project_id)
    .fetch_optional(&deployment.db().pool)
    .await?
    .flatten();

    if let Some(dir) = repo_working_dir
        .as_deref()
        .map(str::trim)
        .filter(|dir| !dir.is_empty())
    {
        return Ok(PathBuf::from(dir));
    }

    Err(ApiError::BadRequest(format!(
        "Could not determine working directory for project {}",
        workflow.project_id
    )))
}

async fn refresh_prompt_watcher_registrations(deployment: &DeploymentImpl, workflow_id: &str) {
    let terminals = match Terminal::find_by_workflow(&deployment.db().pool, workflow_id).await {
        Ok(terminals) => terminals,
        Err(e) => {
            tracing::warn!(
                workflow_id = %workflow_id,
                error = %e,
                "Failed to load terminals for prompt watcher refresh"
            );
            return;
        }
    };

    if terminals.is_empty() {
        return;
    }

    let tasks = match WorkflowTask::find_by_workflow(&deployment.db().pool, workflow_id).await {
        Ok(tasks) => tasks,
        Err(e) => {
            tracing::warn!(
                workflow_id = %workflow_id,
                error = %e,
                "Failed to load tasks for prompt watcher refresh"
            );
            return;
        }
    };

    let workflow_by_task: HashMap<String, String> = tasks
        .into_iter()
        .map(|task| (task.id, task.workflow_id))
        .collect();

    for terminal in terminals {
        let Some(session_id) = terminal
            .pty_session_id
            .clone()
            .filter(|session_id| !session_id.trim().is_empty())
        else {
            tracing::warn!(
                workflow_id = %workflow_id,
                terminal_id = %terminal.id,
                "Skipped prompt watcher refresh registration: missing pty_session_id"
            );
            continue;
        };

        let resolved_workflow_id = workflow_by_task
            .get(&terminal.workflow_task_id)
            .cloned()
            .unwrap_or_else(|| workflow_id.to_string());

        if let Err(e) = deployment
            .prompt_watcher()
            .register(
                &terminal.id,
                &resolved_workflow_id,
                &terminal.workflow_task_id,
                &session_id,
                terminal.auto_confirm,
            )
            .await
        {
            tracing::warn!(
                workflow_id = %resolved_workflow_id,
                terminal_id = %terminal.id,
                task_id = %terminal.workflow_task_id,
                error = %e,
                "Failed to refresh prompt watcher registration"
            );
        }
    }
}

/// POST /api/workflows/:workflow_id/prepare
/// Prepare workflow: start all terminals (created → starting → ready)
///
/// This endpoint performs serial model switching for all terminals using cc-switch,
/// then transitions the workflow to "ready" status for user confirmation before execution.
async fn prepare_workflow(
    State(deployment): State<DeploymentImpl>,
    Path(workflow_id): Path<String>,
) -> Result<ResponseJson<ApiResponse<()>>, ApiError> {
    // Check workflow exists
    let workflow = Workflow::find_by_id(&deployment.db().pool, &workflow_id)
        .await?
        .ok_or_else(|| ApiError::NotFound("Workflow not found".to_string()))?;

    // Verify workflow is in "created" or "failed" status (can retry failed workflows)
    if workflow.status != "created" && workflow.status != "failed" {
        return Err(ApiError::BadRequest(format!(
            "Cannot prepare workflow: current status is '{}', expected 'created' or 'failed'",
            workflow.status
        )));
    }

    // Update status to "starting"
    Workflow::update_status(&deployment.db().pool, &workflow_id, "starting").await?;

    // Create services for terminal coordination
    // Note: Model configuration is now handled at spawn time via environment variable injection,
    // not by the coordinator. This provides process-level isolation for concurrent workflows.
    let db_arc = Arc::new(deployment.db().clone());
    let coordinator =
        TerminalCoordinator::with_message_bus(db_arc.clone(), deployment.message_bus().clone());

    // Step 1: Transition terminals to "starting" status using TerminalCoordinator
    if let Err(e) = coordinator.start_terminals_for_workflow(&workflow_id).await {
        // Log the error for debugging
        tracing::error!(
            workflow_id = %workflow_id,
            error = %e,
            "Failed to prepare workflow terminals"
        );

        rollback_prepare_failure(&deployment, &workflow_id, "terminal preparation failed").await;

        return Err(ApiError::Internal(format!(
            "Failed to prepare workflow terminals: {e}"
        )));
    }

    // Step 2: Resolve working directory.
    // Priority: project.default_agent_working_dir -> first project repo path.
    let working_dir = match resolve_workflow_working_dir(&deployment, &workflow).await {
        Ok(path) => path,
        Err(e) => {
            rollback_prepare_failure(
                &deployment,
                &workflow_id,
                "working directory resolution failed during prepare",
            )
            .await;
            return Err(e);
        }
    };

    // Step 3: Launch PTY processes for all terminals
    let cc_switch = Arc::new(CCSwitchService::new(db_arc.clone()));
    let process_manager = deployment.process_manager().clone();
    let message_bus = deployment.message_bus().clone();
    let prompt_watcher = deployment.prompt_watcher().clone();
    let launcher = TerminalLauncher::with_message_bus(
        db_arc,
        cc_switch,
        process_manager,
        working_dir,
        message_bus,
        prompt_watcher,
    );

    let launch_results = match launcher.launch_all(&workflow_id).await {
        Ok(results) => results,
        Err(e) => {
            tracing::error!(
                workflow_id = %workflow_id,
                error = %e,
                "Failed to launch workflow terminals"
            );

            rollback_prepare_failure(
                &deployment,
                &workflow_id,
                "terminal launch error during prepare",
            )
            .await;

            return Err(ApiError::Internal(format!(
                "Failed to launch workflow terminals: {e}"
            )));
        }
    };

    // Check if any terminal failed to launch
    // Extract error info before any await to avoid Send issues with LaunchResult
    let failed_error_msgs: Vec<String> = launch_results
        .iter()
        .filter(|r| !r.success)
        .map(|r| {
            format!(
                "{}: {}",
                r.terminal_id,
                r.error.as_deref().unwrap_or("unknown")
            )
        })
        .collect();

    let launched_count = launch_results.len();
    drop(launch_results); // Release non-Send types before await

    if !failed_error_msgs.is_empty() {
        tracing::error!(
            workflow_id = %workflow_id,
            failed_count = failed_error_msgs.len(),
            errors = ?failed_error_msgs,
            "Some terminals failed to launch"
        );

        rollback_prepare_failure(&deployment, &workflow_id, "partial terminal launch failure")
            .await;

        return Err(ApiError::Internal(format!(
            "Failed to launch terminals: {}",
            failed_error_msgs.join(", ")
        )));
    }

    tracing::info!(
        workflow_id = %workflow_id,
        launched_count = launched_count,
        "All terminals launched successfully"
    );

    // All terminals prepared successfully, mark workflow as ready
    if let Err(e) = Workflow::set_ready(&deployment.db().pool, &workflow_id).await {
        tracing::error!(
            workflow_id = %workflow_id,
            error = %e,
            "Failed to set workflow ready status"
        );

        rollback_prepare_failure(
            &deployment,
            &workflow_id,
            "failed to finalize prepare status",
        )
        .await;

        return Err(ApiError::Internal(
            "Failed to finalize workflow preparation".to_string(),
        ));
    }

    tracing::info!(
        workflow_id = %workflow_id,
        "Workflow prepared and ready for execution"
    );

    Ok(ResponseJson(ApiResponse::success(())))
}

/// POST /api/workflows/:workflow_id/start
/// Start workflow (user confirmed) or resume from paused state
async fn start_workflow(
    State(deployment): State<DeploymentImpl>,
    Path(workflow_id): Path<String>,
) -> Result<ResponseJson<ApiResponse<()>>, ApiError> {
    // Check workflow exists
    let mut workflow = Workflow::find_by_id(&deployment.db().pool, &workflow_id)
        .await?
        .ok_or_else(|| ApiError::NotFound("Workflow not found".to_string()))?;

    // Recover stale workflow status after restart: DB may still be `running`
    // while runtime instance is no longer active.
    if workflow.status == "running"
        && !deployment
            .orchestrator_runtime()
            .is_running(&workflow_id)
            .await
    {
        tracing::warn!(
            workflow_id = %workflow_id,
            "Workflow marked running but runtime is not active; recovering to paused"
        );
        Workflow::update_status(&deployment.db().pool, &workflow_id, "paused").await?;
        workflow = Workflow::find_by_id(&deployment.db().pool, &workflow_id)
            .await?
            .ok_or_else(|| ApiError::NotFound("Workflow not found".to_string()))?;
    }

    // Self-heal for restarted backend: a workflow may still be `ready` while terminals were
    // reconciled to `not_started` (missing active PTY/session). Re-prepare before starting.
    if workflow.status == "ready" || workflow.status == "paused" {
        let terminals =
            db::models::Terminal::find_by_workflow(&deployment.db().pool, &workflow_id).await?;

        let needs_reprepare = terminals.iter().any(|terminal| {
            terminal.status != "waiting"
                || terminal
                    .pty_session_id
                    .as_deref()
                    .map(str::trim)
                    .filter(|session| !session.is_empty())
                    .is_none()
        });

        if needs_reprepare {
            tracing::warn!(
                workflow_id = %workflow_id,
                workflow_status = %workflow.status,
                "Workflow terminals are not launch-ready; re-preparing before start"
            );

            Workflow::update_status(&deployment.db().pool, &workflow_id, "created").await?;

            let _ = prepare_workflow(State(deployment.clone()), Path(workflow_id.clone())).await?;

            workflow = Workflow::find_by_id(&deployment.db().pool, &workflow_id)
                .await?
                .ok_or_else(|| ApiError::NotFound("Workflow not found".to_string()))?;
        }
    }

    // Verify orchestrator is enabled (only check needed at API level)
    if !workflow.orchestrator_enabled {
        return Err(ApiError::BadRequest(
            "Cannot start workflow: orchestrator is not enabled".to_string(),
        ));
    }

    // Validate workflow status - allow starting from ready or resuming from paused
    let valid_start_statuses = ["ready", "paused"];
    if !valid_start_statuses.contains(&workflow.status.as_str()) {
        return Err(ApiError::BadRequest(format!(
            "Cannot start workflow: current status is '{}', expected 'ready' or 'paused'",
            workflow.status
        )));
    }

    // If resuming from paused, first reset to ready state
    if workflow.status == "paused" {
        Workflow::update_status(&deployment.db().pool, &workflow_id, "ready").await?;
        tracing::info!(workflow_id = %workflow_id, "Resuming workflow from paused state");
    }

    // Call orchestrator runtime to start workflow
    // Runtime handles all status validation atomically
    deployment
        .orchestrator_runtime()
        .start_workflow(&workflow_id)
        .await
        .map_err(|e| {
            // Log full error internally
            tracing::error!("Failed to start workflow {}: {:?}", workflow_id, e);
            // Return generic message to client
            ApiError::Internal(format!("Failed to start workflow"))
        })?;

    refresh_prompt_watcher_registrations(&deployment, &workflow_id).await;

    // Note: Workflow::set_started is called inside OrchestratorRuntime::start_workflow
    // to ensure the status update happens atomically with runtime startup

    Ok(ResponseJson(ApiResponse::success(())))
}

/// POST /api/workflows/:workflow_id/pause
/// Pause a running workflow
async fn pause_workflow(
    State(deployment): State<DeploymentImpl>,
    Path(workflow_id): Path<String>,
) -> Result<ResponseJson<ApiResponse<()>>, ApiError> {
    // Check workflow exists
    let workflow = Workflow::find_by_id(&deployment.db().pool, &workflow_id)
        .await?
        .ok_or_else(|| ApiError::NotFound("Workflow not found".to_string()))?;

    // Only allow pausing a running workflow
    if workflow.status != "running" {
        return Err(ApiError::BadRequest(format!(
            "Cannot pause workflow: current status is '{}', expected 'running'",
            workflow.status
        )));
    }

    // Stop the orchestrator runtime if it's active
    let runtime = deployment.orchestrator_runtime();
    if runtime.is_running(&workflow_id).await {
        runtime.stop_workflow(&workflow_id).await.map_err(|e| {
            tracing::error!("Failed to stop workflow {} for pause: {:?}", workflow_id, e);
            ApiError::Internal("Failed to pause workflow".to_string())
        })?;
    }

    // Mark workflow as paused
    Workflow::update_status(&deployment.db().pool, &workflow_id, "paused").await?;

    tracing::info!(
        workflow_id = %workflow_id,
        "Workflow paused"
    );

    Ok(ResponseJson(ApiResponse::success(())))
}

/// POST /api/workflows/:workflow_id/stop
/// Stop a workflow and mark as cancelled
async fn stop_workflow(
    State(deployment): State<DeploymentImpl>,
    Path(workflow_id): Path<String>,
) -> Result<ResponseJson<ApiResponse<()>>, ApiError> {
    // Check workflow exists
    let workflow = Workflow::find_by_id(&deployment.db().pool, &workflow_id)
        .await?
        .ok_or_else(|| ApiError::NotFound("Workflow not found".to_string()))?;

    // Allow stopping from starting/running/paused
    let valid_statuses = ["starting", "running", "paused"];
    if !valid_statuses.contains(&workflow.status.as_str()) {
        return Err(ApiError::BadRequest(format!(
            "Cannot stop workflow: current status is '{}', expected one of: {:?}",
            workflow.status, valid_statuses
        )));
    }

    stop_workflow_runtime_if_running(
        &deployment,
        &workflow_id,
        "stopping workflow",
        "Failed to stop workflow",
    )
    .await?;
    let terminals =
        cleanup_workflow_terminals(&deployment, &workflow_id, "stopping workflow").await?;

    // Mark workflow as cancelled
    Workflow::update_status(&deployment.db().pool, &workflow_id, "cancelled").await?;

    // Mark all tasks as cancelled
    let tasks = WorkflowTask::find_by_workflow(&deployment.db().pool, &workflow_id).await?;
    for task in &tasks {
        if task.status != "completed" {
            WorkflowTask::update_status(&deployment.db().pool, &task.id, "cancelled").await?;
        }
    }

    // Mark all terminals as cancelled
    for terminal in &terminals {
        if terminal.status != "completed" {
            Terminal::update_status(&deployment.db().pool, &terminal.id, "cancelled").await?;
            Terminal::update_process(&deployment.db().pool, &terminal.id, None, None).await?;
        }
    }

    tracing::info!(
        workflow_id = %workflow_id,
        "Workflow stopped and cancelled"
    );

    Ok(ResponseJson(ApiResponse::success(())))
}

async fn stop_workflow_runtime_if_running(
    deployment: &DeploymentImpl,
    workflow_id: &str,
    action: &str,
    internal_error_message: &str,
) -> Result<(), ApiError> {
    let runtime = deployment.orchestrator_runtime();
    if runtime.is_running(workflow_id).await {
        runtime.stop_workflow(workflow_id).await.map_err(|e| {
            tracing::error!(
                workflow_id = %workflow_id,
                action = action,
                error = ?e,
                "Failed to stop workflow runtime"
            );
            ApiError::Internal(internal_error_message.to_string())
        })?;
    }
    Ok(())
}

async fn cleanup_workflow_terminals(
    deployment: &DeploymentImpl,
    workflow_id: &str,
    action: &str,
) -> Result<Vec<Terminal>, ApiError> {
    let pool = &deployment.db().pool;
    let terminals = Terminal::find_by_workflow(pool, workflow_id).await?;
    for terminal in &terminals {
        if let Err(e) = deployment
            .process_manager()
            .kill_terminal(&terminal.id)
            .await
        {
            tracing::warn!(
                terminal_id = %terminal.id,
                workflow_id = %workflow_id,
                action = action,
                error = %e,
                "Failed to kill terminal process during workflow cleanup"
            );
        }

        deployment.prompt_watcher().unregister(&terminal.id).await;
    }

    Ok(terminals)
}

/// POST /api/workflows/recover
/// Trigger recovery of workflows after service restart
async fn recover_workflows(
    State(_deployment): State<DeploymentImpl>,
) -> Result<ResponseJson<ApiResponse<RecoveryResponse>>, ApiError> {
    // In a full implementation, this would:
    // 1. Scan database for workflows in "running" state
    // 2. Reconnect orchestrator runtime to those workflows
    // 3. Verify terminal/session states
    // 4. Update statuses for workflows that terminated during restart

    let response = RecoveryResponse {
        message: "Recovery triggered".to_string(),
    };

    Ok(ResponseJson(ApiResponse::success(response)))
}

/// GET /api/workflows/:workflow_id/tasks
/// List workflow tasks
async fn list_workflow_tasks(
    State(deployment): State<DeploymentImpl>,
    Path(workflow_id): Path<String>,
) -> Result<ResponseJson<ApiResponse<Vec<WorkflowTaskDetailResponse>>>, ApiError> {
    let tasks = WorkflowTask::find_by_workflow(&deployment.db().pool, &workflow_id).await?;
    let mut task_details = Vec::new();
    for task in tasks {
        let terminals = Terminal::find_by_task(&deployment.db().pool, &task.id).await?;
        task_details.push(WorkflowTaskDetailResponse { task, terminals });
    }
    Ok(ResponseJson(ApiResponse::success(task_details)))
}

/// Request body for updating task status
#[derive(Debug, Deserialize)]
pub struct UpdateTaskStatusRequest {
    pub status: String,
}

/// Request body for submitting interactive prompt response
#[derive(Debug, Deserialize)]
pub struct SubmitPromptResponseRequest {
    #[serde(rename = "terminalId", alias = "terminal_id")]
    pub terminal_id: String,
    pub response: String,
}

/// PUT /api/workflows/:workflow_id/tasks/:task_id/status
/// Update task status (for Kanban drag-and-drop)
async fn update_task_status(
    State(deployment): State<DeploymentImpl>,
    Path((workflow_id, task_id)): Path<(String, String)>,
    Json(req): Json<UpdateTaskStatusRequest>,
) -> Result<ResponseJson<ApiResponse<WorkflowTask>>, ApiError> {
    // Verify workflow exists
    let _workflow = Workflow::find_by_id(&deployment.db().pool, &workflow_id)
        .await?
        .ok_or_else(|| ApiError::NotFound("Workflow not found".to_string()))?;

    // Verify task exists and belongs to the workflow
    let task = WorkflowTask::find_by_id(&deployment.db().pool, &task_id)
        .await?
        .ok_or_else(|| ApiError::NotFound("Task not found".to_string()))?;

    validate_task_workflow_scope(&task, &workflow_id)?;

    // Validate status value - support both backend and frontend status names
    let valid_statuses = [
        "pending",        // Initial state
        "running",        // Task is being worked on
        "review_pending", // Awaiting review
        "completed",      // Task completed successfully
        "failed",         // Task failed
        "cancelled",      // Task was cancelled
    ];
    if !valid_statuses.contains(&req.status.as_str()) {
        return Err(ApiError::BadRequest(format!(
            "Invalid status '{}', expected one of: {:?}",
            req.status, valid_statuses
        )));
    }

    // Update task status
    WorkflowTask::update_status(&deployment.db().pool, &task_id, &req.status).await?;

    // Fetch updated task
    let updated_task = WorkflowTask::find_by_id(&deployment.db().pool, &task_id)
        .await?
        .ok_or_else(|| ApiError::Internal("Failed to fetch updated task".to_string()))?;

    tracing::info!(
        workflow_id = %workflow_id,
        task_id = %task_id,
        new_status = %req.status,
        "Task status updated"
    );

    Ok(ResponseJson(ApiResponse::success(updated_task)))
}

/// GET /api/workflows/:workflow_id/tasks/:task_id/terminals
/// List task terminals
async fn list_task_terminals(
    State(deployment): State<DeploymentImpl>,
    Path((workflow_id, task_id)): Path<(String, String)>,
) -> Result<ResponseJson<ApiResponse<Vec<Terminal>>>, ApiError> {
    let _workflow = Workflow::find_by_id(&deployment.db().pool, &workflow_id)
        .await?
        .ok_or_else(|| ApiError::NotFound("Workflow not found".to_string()))?;

    let task = WorkflowTask::find_by_id(&deployment.db().pool, &task_id)
        .await?
        .ok_or_else(|| ApiError::NotFound("Task not found".to_string()))?;

    validate_task_workflow_scope(&task, &workflow_id)?;

    let terminals = Terminal::find_by_task(&deployment.db().pool, &task_id).await?;
    Ok(ResponseJson(ApiResponse::success(terminals)))
}

/// POST /api/workflows/:workflow_id/prompts/respond
/// Submit user response for interactive terminal prompt
async fn submit_prompt_response(
    State(deployment): State<DeploymentImpl>,
    Path(workflow_id): Path<String>,
    Json(payload): Json<SubmitPromptResponseRequest>,
) -> Result<ResponseJson<ApiResponse<()>>, ApiError> {
    let terminal_id = payload.terminal_id.trim();
    if terminal_id.is_empty() {
        return Err(ApiError::BadRequest("terminalId is required".to_string()));
    }

    let response = payload.response.as_str();

    let _workflow = Workflow::find_by_id(&deployment.db().pool, &workflow_id)
        .await?
        .ok_or_else(|| ApiError::NotFound("Workflow not found".to_string()))?;

    let terminal = Terminal::find_by_id(&deployment.db().pool, terminal_id)
        .await?
        .ok_or_else(|| ApiError::NotFound("Terminal not found".to_string()))?;

    let task = WorkflowTask::find_by_id(&deployment.db().pool, &terminal.workflow_task_id)
        .await?
        .ok_or_else(|| ApiError::NotFound("Workflow task not found".to_string()))?;

    validate_task_workflow_scope(&task, &workflow_id)?;

    let runtime = deployment.orchestrator_runtime();
    if !runtime.is_running(&workflow_id).await {
        return Err(ApiError::BadRequest(format!(
            "Cannot submit prompt response: workflow '{}' is not running",
            workflow_id
        )));
    }

    runtime
        .submit_user_prompt_response(&workflow_id, terminal_id, response)
        .await
        .map_err(|e| {
            tracing::warn!(
                workflow_id = %workflow_id,
                terminal_id = %terminal_id,
                error = %e,
                "Failed to submit prompt response"
            );
            ApiError::BadRequest(format!("Failed to submit prompt response: {e}"))
        })?;

    tracing::info!(
        workflow_id = %workflow_id,
        terminal_id = %terminal_id,
        "Submitted prompt response"
    );

    Ok(ResponseJson(ApiResponse::success(())))
}

/// POST /api/workflows/:workflow_id/merge
/// Execute merge terminal for workflow
async fn merge_workflow(
    State(deployment): State<DeploymentImpl>,
    Path(workflow_id): Path<String>,
    Json(payload): Json<MergeWorkflowRequest>,
) -> Result<ResponseJson<ApiResponse<serde_json::Value>>, ApiError> {
    if let Some(strategy) = payload.merge_strategy.as_deref()
        && !strategy.eq_ignore_ascii_case("squash")
    {
        return Err(ApiError::BadRequest(format!(
            "Unsupported merge strategy '{}': only 'squash' is supported",
            strategy
        )));
    }

    // Check workflow exists
    let workflow = Workflow::find_by_id(&deployment.db().pool, &workflow_id)
        .await?
        .ok_or_else(|| ApiError::NotFound("Workflow not found".to_string()))?;

    // Validate workflow is in appropriate state for merging
    let current_status = workflow.status.as_str();
    if !can_merge_from_workflow_status(current_status) {
        return Err(ApiError::BadRequest(format!(
            "Cannot merge workflow with status '{}': expected one of: {:?}",
            current_status, MERGE_ALLOWED_WORKFLOW_STATUSES
        )));
    }

    let project = Project::find_by_id(&deployment.db().pool, workflow.project_id)
        .await?
        .ok_or_else(|| ApiError::NotFound("Project not found".to_string()))?;

    let base_repo_path = project
        .default_agent_working_dir
        .as_deref()
        .map(str::trim)
        .filter(|path| !path.is_empty())
        .map(PathBuf::from)
        .ok_or_else(|| {
            ApiError::BadRequest(
                "Cannot merge workflow: project has no default agent working directory".to_string(),
            )
        })?;

    if !base_repo_path.exists() {
        return Err(ApiError::BadRequest(format!(
            "Cannot merge workflow: base repository path does not exist ({})",
            base_repo_path.display()
        )));
    }

    let tasks = WorkflowTask::find_by_workflow(&deployment.db().pool, &workflow_id).await?;
    if tasks.is_empty() {
        return Err(ApiError::BadRequest(
            "Cannot merge workflow: no tasks found".to_string(),
        ));
    }

    let unfinished_tasks: Vec<String> = tasks
        .iter()
        .filter(|task| task.status != "completed")
        .map(|task| format!("{}({})", task.id, task.status))
        .collect();

    if !unfinished_tasks.is_empty() {
        return Err(ApiError::Conflict(format!(
            "Cannot merge workflow: unfinished tasks found [{}]",
            unfinished_tasks.join(", ")
        )));
    }

    Workflow::update_status(&deployment.db().pool, &workflow_id, "merging").await?;

    let mut merged_tasks = Vec::new();
    for task in tasks {
        let task_id = task.id.clone();
        let task_branch = task.branch.trim();
        if task_branch.is_empty() {
            let _ = Workflow::update_status(&deployment.db().pool, &workflow_id, "failed").await;
            return Err(ApiError::BadRequest(format!(
                "Cannot merge task {}: branch is empty",
                task_id
            )));
        }

        let task_worktree_path = base_repo_path.join("worktrees").join(task_branch);
        if !task_worktree_path.exists() {
            let _ = Workflow::update_status(&deployment.db().pool, &workflow_id, "failed").await;
            return Err(ApiError::BadRequest(format!(
                "Cannot merge task {}: worktree path does not exist ({})",
                task_id,
                task_worktree_path.display()
            )));
        }

        let commit_message = format!("Merge task {} ({})", task_id, task_branch);
        match deployment.git().merge_changes(
            &base_repo_path,
            &task_worktree_path,
            task_branch,
            &workflow.target_branch,
            &commit_message,
        ) {
            Ok(commit_sha) => {
                merged_tasks.push(json!({
                    "taskId": task_id,
                    "branch": task_branch,
                    "commitSha": commit_sha,
                }));
            }
            Err(err) => {
                let should_keep_merging_status = matches!(
                    &err,
                    GitServiceError::MergeConflicts(_)
                        | GitServiceError::BranchesDiverged(_)
                        | GitServiceError::WorktreeDirty(_, _)
                        | GitServiceError::RebaseInProgress
                );

                let fallback_status = if should_keep_merging_status {
                    "merging"
                } else {
                    "failed"
                };
                if let Err(status_err) =
                    Workflow::update_status(&deployment.db().pool, &workflow_id, fallback_status)
                        .await
                {
                    tracing::warn!(
                        workflow_id = %workflow_id,
                        status = fallback_status,
                        error = %status_err,
                        "Failed to update workflow status after merge failure"
                    );
                }

                return Err(ApiError::from(err));
            }
        }
    }

    Workflow::update_status(&deployment.db().pool, &workflow_id, "completed").await?;

    // Return success response
    let result = json!({
        "success": true,
        "message": "Merge completed successfully",
        "workflow_id": workflow_id,
        "workflowId": workflow_id,
        "targetBranch": workflow.target_branch,
        "mergedTasks": merged_tasks
    });

    Ok(ResponseJson(ApiResponse::success(result)))
}

// ============================================================================
// Contract Tests
// ============================================================================

#[cfg(test)]
mod dto_tests {
    use super::*;
    use crate::routes::workflows_dto::WorkflowDetailDto;

    #[test]
    fn test_list_workflows_returns_camelcase() {
        // This test validates the expected format
        let response_json = r#"[
            {
                "id": "wf-test",
                "projectId": "proj-test",
                "name": "Test",
                "status": "created",
                "createdAt": "2026-01-24T10:00:00Z",
                "updatedAt": "2026-01-24T10:00:00Z",
                "tasksCount": 0,
                "terminalsCount": 0
            }
        ]"#;

        // Verify no snake_case
        assert!(!response_json.contains("\"project_id\""));
        assert!(!response_json.contains("\"created_at\""));

        // Verify camelCase
        assert!(response_json.contains("\"projectId\""));
        assert!(response_json.contains("\"createdAt\""));
    }

    #[test]
    fn test_get_workflow_returns_camelcase() {
        let response_json = r#"{
            "id": "wf-test",
            "projectId": "proj-test",
            "name": "Test Workflow",
            "status": "created",
            "useSlashCommands": true,
            "orchestratorEnabled": true,
            "createdAt": "2026-01-24T10:00:00Z",
            "updatedAt": "2026-01-24T10:00:00Z",
            "tasks": [],
            "commands": []
        }"#;

        // Verify no snake_case
        assert!(!response_json.contains("\"project_id\""));
        assert!(!response_json.contains("\"use_slash_commands\""));

        // Verify camelCase
        assert!(response_json.contains("\"projectId\""));
        assert!(response_json.contains("\"useSlashCommands\""));
        assert!(response_json.contains("\"orchestratorEnabled\""));
    }
}

#[cfg(test)]
mod workflow_guard_tests {
    use chrono::Utc;

    use super::*;

    fn build_task_with_workflow(workflow_id: &str) -> WorkflowTask {
        WorkflowTask {
            id: "task-test".to_string(),
            workflow_id: workflow_id.to_string(),
            vk_task_id: None,
            name: "Task".to_string(),
            description: None,
            branch: "workflow/test/task".to_string(),
            status: "pending".to_string(),
            order_index: 0,
            started_at: None,
            completed_at: None,
            created_at: Utc::now(),
            updated_at: Utc::now(),
        }
    }

    #[test]
    fn merge_status_guard_allows_only_terminal_merge_states() {
        assert!(can_merge_from_workflow_status("completed"));
        assert!(can_merge_from_workflow_status("merging"));
        assert!(!can_merge_from_workflow_status("starting"));
        assert!(!can_merge_from_workflow_status("running"));
    }

    #[test]
    fn merge_status_guard_rejects_all_non_merge_states() {
        let non_merge_states = [
            "created",
            "starting",
            "ready",
            "running",
            "paused",
            "failed",
            "cancelled",
        ];

        for status in non_merge_states {
            assert!(
                !can_merge_from_workflow_status(status),
                "Expected status '{status}' to be rejected by merge guard"
            );
        }
    }

    #[test]
    fn workflow_status_transition_accepts_valid_paths() {
        let allowed_cases = [
            ("created", "starting"),
            ("starting", "ready"),
            ("ready", "running"),
            ("running", "paused"),
            ("running", "completed"),
            ("paused", "ready"),
            ("completed", "merging"),
            ("merging", "completed"),
            ("failed", "starting"),
            ("cancelled", "created"),
            ("completed", "created"),
        ];

        for (current, next) in allowed_cases {
            assert!(
                validate_workflow_status_transition(current, next).is_ok(),
                "Expected transition {current} -> {next} to be valid"
            );
        }
    }

    #[test]
    fn workflow_status_transition_rejects_illegal_jumps() {
        let rejected_cases = [
            ("created", "running"),
            ("starting", "completed"),
            ("ready", "completed"),
            ("completed", "running"),
            ("merging", "running"),
            ("cancelled", "running"),
        ];

        for (current, next) in rejected_cases {
            assert!(
                matches!(
                    validate_workflow_status_transition(current, next),
                    Err(ApiError::Conflict(_))
                ),
                "Expected transition {current} -> {next} to be rejected"
            );
        }

        assert!(matches!(
            validate_workflow_status_transition("created", "unknown_status"),
            Err(ApiError::BadRequest(_))
        ));
    }

    #[test]
    fn task_scope_guard_rejects_cross_workflow_access() {
        let task = build_task_with_workflow("wf-1");

        assert!(validate_task_workflow_scope(&task, "wf-1").is_ok());
        assert!(matches!(
            validate_task_workflow_scope(&task, "wf-2"),
            Err(ApiError::BadRequest(_))
        ));
    }
}

#[cfg(test)]
mod prompt_response_route_tests {
    use axum::{
        body::{Body, to_bytes},
        http::{Request, StatusCode},
    };
    use deployment::Deployment;
    use serde_json::json;
    use serial_test::serial;
    use tower::ServiceExt;

    use super::*;

    #[tokio::test]
    #[serial]
    async fn submit_prompt_response_requires_terminal_id() {
        let deployment = DeploymentImpl::new()
            .await
            .expect("Failed to create deployment");

        let app = workflows_routes().with_state(deployment);
        let payload = json!({
            "terminalId": "   ",
            "response": "yes"
        })
        .to_string();

        let request = Request::builder()
            .method("POST")
            .uri("/wf-test/prompts/respond")
            .header("content-type", "application/json")
            .body(Body::from(payload))
            .expect("Failed to build request");

        let response = app
            .oneshot(request)
            .await
            .expect("Failed to execute request");

        assert_eq!(response.status(), StatusCode::BAD_REQUEST);

        let body = to_bytes(response.into_body(), usize::MAX)
            .await
            .expect("Failed to read response body");
        let body_json: serde_json::Value =
            serde_json::from_slice(&body).expect("Failed to parse response JSON");
        assert_eq!(
            body_json.get("message").and_then(serde_json::Value::as_str),
            Some("terminalId is required")
        );
    }

    #[tokio::test]
    #[serial]
    async fn submit_prompt_response_allows_empty_response() {
        let deployment = DeploymentImpl::new()
            .await
            .expect("Failed to create deployment");

        let app = workflows_routes().with_state(deployment);
        let payload = json!({
            "terminalId": "terminal-1",
            "response": ""
        })
        .to_string();

        let request = Request::builder()
            .method("POST")
            .uri("/wf-test/prompts/respond")
            .header("content-type", "application/json")
            .body(Body::from(payload))
            .expect("Failed to build request");

        let response = app
            .oneshot(request)
            .await
            .expect("Failed to execute request");

        assert_eq!(response.status(), StatusCode::NOT_FOUND);

        let body = to_bytes(response.into_body(), usize::MAX)
            .await
            .expect("Failed to read response body");
        let body_json: serde_json::Value =
            serde_json::from_slice(&body).expect("Failed to parse response JSON");
        assert_eq!(
            body_json.get("message").and_then(serde_json::Value::as_str),
            Some("Workflow not found")
        );
    }

    #[tokio::test]
    #[serial]
    async fn submit_prompt_response_requires_response_field() {
        let deployment = DeploymentImpl::new()
            .await
            .expect("Failed to create deployment");

        let app = workflows_routes().with_state(deployment);
        let payload = json!({
            "terminalId": "terminal-1"
        })
        .to_string();

        let request = Request::builder()
            .method("POST")
            .uri("/wf-test/prompts/respond")
            .header("content-type", "application/json")
            .body(Body::from(payload))
            .expect("Failed to build request");

        let response = app
            .oneshot(request)
            .await
            .expect("Failed to execute request");

        assert_eq!(response.status(), StatusCode::UNPROCESSABLE_ENTITY);
    }
}
